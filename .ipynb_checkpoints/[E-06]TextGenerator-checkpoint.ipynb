{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 패키지 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['When somebody loved me', 'Everything was beautiful', 'Every hour we spent together', 'Lives within my heart And when she was sad', 'I was there to dry her tears', 'And when she was happy so was I', 'When she loved me Through the summer and the fall', 'We had each other that was all', 'Just she and I together', 'Like it was meant to be And when she was lonely']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  45 246 ...   0   0   0]\n",
      " [  2 177  53 ...   0   0   0]\n",
      " [  2 124 860 ...   0   0   0]\n",
      " ...\n",
      " [  2   8   4 ...   0   0   0]\n",
      " [  2  44  17 ...   0   0   0]\n",
      " [  2   6 172 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f54b7bbb8d0>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "  \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    processed = preprocess_sentence(sentence)\n",
    "    if len(processed.split()) <= 15:\n",
    "        corpus.append(processed)\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, shuffle=True)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 객체 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(src_input, tgt_input):\n",
    "    BUFFER_SIZE = len(src_input)\n",
    "    BATCH_SIZE = 256\n",
    "    steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "    VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "    return dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "train_dataset = create_dataset(enc_train, dec_train)\n",
    "test_dataset = create_dataset(enc_val, dec_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  12289024  \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                multiple                  25174016  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  24590049  \n",
      "=================================================================\n",
      "Total params: 95,615,713\n",
      "Trainable params: 95,615,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 1024\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "for src_sample, tgt_sample in train_dataset.take(1): break\n",
    "model(src_sample)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 253s 519ms/step - loss: 3.2250\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 265s 544ms/step - loss: 2.6908\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 269s 552ms/step - loss: 2.3680\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 270s 555ms/step - loss: 2.0528\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 269s 553ms/step - loss: 1.7574\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 258s 529ms/step - loss: 1.4976\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 265s 544ms/step - loss: 1.2906\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 271s 557ms/step - loss: 1.1396\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 255s 524ms/step - loss: 1.0455\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 253s 519ms/step - loss: 0.9953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f54a0066950>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 17s 143ms/step - loss: 2.1720\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
    "\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated\n",
    "\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회고   \n",
    "\n",
    "**목표**\n",
    "   \n",
    "모델을 수정하지 않고 embedding_size, hidden_size만 수정해서 validation loss를 10 epochs 이내에 2.2 이하로 떨어뜨리기\n",
    "\n",
    "**새로 알게된 것**   \n",
    "\n",
    "1. 지금까지 numpy array 타입의 데이터와 라벨을 model의 fit 메서드에 전달해 학습을 해왔는데, 텐서플로우에선 이보다는 Dataset객체를 생성해 사용하는 것이 속도도 더 빠르고 편리하다고 한다. 정확하게 어떻게 빠르고 편리한지 찾아봐야 할 것 같다.   \n",
    "\n",
    "2. tf.keras.Sequential()로 모델을 생성하고, add 메소드로 레이어를 추가하는 방식이 아니라 클래스와 메소드를 통해 모델을 생성하는 방법을 처음 사용해봤다.(https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n",
    "\n",
    "**과정**\n",
    "\n",
    "해당 모델로 validation loss를 10 epochs 이내에 2.2 이하로 떨어뜨리는 것이 목표였다.\n",
    "\n",
    "처음엔 embedding_size를 256, hidden_size를 1024로 설정하고 학습을 시켰는데, 10 번째 epoch에서 training loss가 2.3이 나왔다.\n",
    "\n",
    "그 다음엔 hidden_size를 2048로 변경하고 학습을 시켰는데 비슷한 결과가 나왔다.\n",
    "\n",
    "다시 hidden_size를 1024로 되돌리고, embedding_size를 512로 늘렸더니 10 번째 epoch에서 training loss가 2.0에 가까운 값이 나왔다.\n",
    "하지만 validation loss는 2.4를 넘었다.\n",
    "\n",
    "embedding_size를 늘렸을 때 training loss가 떨어진 것에 주목해, embedding_size를 1024, hidden_size를 2048로 설정해봤다.\n",
    "\n",
    "training loss는 1보다 작은 값이 나왔고, validation loss는 2.17이 나왔다.\n",
    "\n",
    "overfitting을 해결해야 하지만, embedding_size, hidden_size의 설정값을 키웠을 때 목표를 달성할 수 있었던 이유에 대해서도 공부가 더 필요할 것 같다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
